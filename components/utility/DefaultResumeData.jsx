const DefaultResumeData = {
  name: "Muhammad Umair Javaid",
  position: "AI Engineer",
  contactInformation: "+923234965395",
  email: "1.umairjavaid@gmail.com",
  address: "Lahore, Pakistan",
  profilePicture: "",
  socialMedia: [
    {
      socialMedia: "Github",
      link: "github.com/umairjavaid",
    },
    {
      socialMedia: "LinkedIn",
      link: "www.linkedin.com/in/muhammad-umair-javaid-02173096/",
    },
    {
      socialMedia: "",
      link: "",
    },
  ],
  summary:
    "Experienced Machine Learning Engineer and Software Infrastructure Specialist, focused on building retrieval-augmented AI systems, optimizing models, and deploying low-latency, real-time solutions. Skilled in ML pipelines, MLOps, DevOps, system design, and best practices like metrics-driven, behavior-driven, and test-driven development to deliver scalable and resilient machine learning systems",
  education: [
    {
      school: "Bachelor of Computer Science",
      degree: "Information Technology University of Punjab",
      startYear: "2013-12-20",
      endYear: "2019-12-01",
    },
  ],
  workExperience: [
    {
      company: "Visionet Systems ltd",
      position: "Senior Data Science Consultant",
      description:
        "Contributed to Visionet’s innovative solutions in telecommunications, large retail, and legal sectors, driving AI, automation, and digital transformation",
      keyAchievements:
        "RAG Implementation for Industry Documents - Designed and implemented a Retrieval-Augmented Generation (RAG) system using Amazon Bedrock, Pinecone, and LangChain, achieving a production-standard accuracy of 93%, which enhanced data retrieval and improved contextual AI response accuracy for industry documents. <a href='https://example.com/rag' target='_blank' rel='noopener noreferrer'>[Link to RAG project]</a>\nChurn Prediction for Telecommunications - Enhanced model quality through extensive regression testing, parameter tuning, and CI/CD integration using Azure DevOps and Azure Automated ML, predicting 7% more churn cases and reducing customer churn by 15%. <a href='https://example.com/churn' target='_blank' rel='noopener noreferrer'>[Link to Churn analysis]</a>\nIdentity Graph Management and Product Recommendation System - Developed and deployed an advanced solution for identity graph management and product recommendation leveraging AWS services (Neptune, Personalize, S3, Glue, API Gateway, Lambda, SageMaker, CloudWatch), successfully clustering 85% of users accurately and increasing sales by 12% through personalized recommendations",
      startYear: "2024-02-16",
      endYear: "",
    },
    {
      company: "Ivy-llc",
      position: "Machine Learning Engineer",
      description:
        "Contributed to Ivy LLC’s open-source Ivy framework, enabling seamless conversion of machine learning models across multiple platforms like PyTorch, TensorFlow, JAX, and PaddlePaddle",
      keyAchievements:
        "Implemented Decision Tree Classifier in Ivy frontend API inspired from HummingBird - achieved a 10x inference time speed up over Sklearn using GPU-parallelizable tree traversal algorithms with Jax Jit compilation. Link\nFixed critical bugs that caused the frontend and transpiler unit tests in CI/CD, Github Actions, to fail for certain frameworks: Link1, Link2\nImplemented unsupported functions in Ivy to enhance code quality, promote collaboration, and advance software development skills through unit testing and continuous integration. Link1, Link2\nProfiled Generative AI(Stable Diffusion) models to identify transpilation bottlenecks using Nvidia Nsight",
      startYear: "2023-08-01",
      endYear: "2024-02-09",
    },
    {
      company: "BRICKandMORTAR.ai",
      position: "Deep Learning Engineer",
      description:
        "Contributed to Brick and Mortar AI (formerly Aletheia AI), delivering deep learning-based analytics for shopping malls, banks, retail, and various industries, transforming CCTV data into actionable insights",
      keyAchievements:
        "Automated industrial truck tracking using Object Detection, Object Tracking, and Text Recognition, optimizing efficiency with 91% accuracy despite diverse truck sizes and non-standardized Pakistani number plates\nCurated diverse datasets with LabelBox, fine-tuned PyTorch, TensorFlow, and PaddlePaddle models, and implemented MLOps practices via Weights & Biases for streamlined automated data capture, analysis, and enhanced efficiency\nAccelerated deep learning models with OpenVino, Nimble, TensorRT, and XLA ensuring efficient inference on CPU and GPUs by optimizing precision, layer fusion, and leveraging the power of concurrent processing for improved real-time performance\nDeveloped a real-time microservices system, low latency and high throughput, utilizing Redis for inter-module communication, with live database updates using SQLite and Flask\nAddressed DevOps considerations such as orchestration, security, monitoring, resource optimization, fault tolerance, and implemented a versioning strategy for updates and rollbacks in the system using Docker, Docker compose and Grafana\nTransformed Python stack into multi-threaded C++ with resource optimization for deployment on Mobile embedded devices (Nvidia Jetson Nano 2gb/4gb) and x86-64 devices",
      startYear: "2022-01-22",
      endYear: "2023-07-31",
    },
    {
      company: "Adlytic AI",
      position: "Machine Learning Engineer",
      description:
        "Contributed to Adlytic AI, developing AI-powered video analytics that transform in-store camera feeds into actionable insights for shopping malls, banks, retail, and industrial sectors.",
      keyAchievements:
        "Developed a real-time Sports Analytics system to generate in-depth analytics on Footfall pass frequency and ball possession using Object Detection and Object Tracking, achieving above 90% accuracy\nDeveloped, implemented and optimized footfall counting, vehicle counting, and attribute classification solutions for retail stores using object detection, Object Tracking and multi-label classification, achieving above 89% accuracy\nDeveloped Modular, Microservices based inference pipeline for deployment on AWS EC2, GCE, Azure VMs\nBoosted inference by 7x through optimization algorithms like Quantization, Pruning & Sparsification, and Knowledge Distillation, using tools such as Nvidia Tao toolkit\nAccelerated deep learning model training using Nvidia TLT",
      startYear: "2020-01-28",
      endYear: "2021-12-28",
    },
  ],
  projects: [],
  skills: [
    {
      title: "Technical Skills",
      skills: [
        "Python",
        "C++",
        "C",
        "Java",
        "Dart",
        "Pytorch",
        "Tensorflow",
        "JAX",
        "PaddlePaddle",
        "scikit-learn",
        "XGBoost",
        "Numpy",
        "Scipy",
        "spaCy",
        "matplotlib",
        "pandas",
        "AWS",
        "Azure",
        "Azure DevOPS",
        "AWS Bedrock",
        "Databricks",
        "Weights & Biases",
        "Hugging Face",
        "TensorBoard",
        "AWS Sagemaker",
        "Git",
        "Docker",
        "Docker Compose",
        "Grafana",
        "SQLite",
        "Redis",
        "Github Actions",
        "Nvidia Nsight",
        "sphinx",
        "Flake8",
        "Flask",
        "Gstreamer",
        "ONNX",
        "TensorRT",
        "Nvidia DeepStream",
        "Git",
        "VScode",
        "EKS",
        "LangChain",
        "Github",
        "PineCone",
      ],
    },
    {
      title: "Soft Skills",
      skills: [
        "Collaboration",
        "Problem-solving",
        "Communication",
        "Time management",
        "Result-oriented",
      ],
    },
    {
      title: "Additional Skills",
      skills: [
        "Public Speaking",
        "Writing",
        "Research",
        "Test-Driven developement",
        "Behavior-Driven Development",
        "Metrics-Driven Development",
      ],
    },
  ],
  languages: ["English", "Hindi", "Urdu", "Punjabi"],
  certifications: [
    "LangChain Mastery: Develop LLM Apps with LangChain, Pinecode, OpenAI and Gemini - Udemy",
    "Azure Machine Leanring & MLOps: Beginner to Advance - Udemy",
    "LLMs Mastery: Complete Guide to Transformers & Generative AI - Udemy",
  ],
  jobDescription: "", // To store the target job description
  llmConfig: {
    apiUrl: "/api/anthropic", // Use the correct local proxy URL
    provider: "simulate", // Always default to simulation mode for safety
    systemPrompt: `You are an expert resume tailoring assistant. Given a resume (in JSON format) and a job description (in text format), rewrite the resume sections (summary, work experience descriptions & key achievements, project descriptions & key achievements) to better match the keywords and requirements in the job description. Make minimal changes, keeping the core meaning and structure intact. Focus on incorporating relevant skills and experiences mentioned in the job description into the existing resume content. Do not invent new experiences or add sections not present in the original JSON. Ensure all original fields like 'name', 'position', 'contactInformation', 'email', 'address', 'profilePicture', 'socialMedia', 'education', 'skills', 'languages', 'certifications', 'llmConfig', 'jobDescription', 'instructionPrompt', 'savedResumes' are preserved exactly as they were in the input JSON. Output ONLY the updated complete resume object in valid JSON format, enclosed within a single \`\`\`json ... \`\`\` block.`,
    userPromptTemplate: `Tailor the following resume:\n\n\`\`\`json\n{resume}\n\`\`\`\n\nTo match this job description:\n\n\`\`\`text\n{job_description}\n\`\`\`\n\nRespond with the updated complete resume in JSON format, enclosed in a markdown code block.`,
    refinePromptTemplate: `Refine the following resume:\n\n\`\`\`json\n{resume}\n\`\`\`\n\nBased on this instruction:\n\n\`\`\`text\n{instruction}\n\`\`\`\n\nFollow the same rules as the initial tailoring: make minimal changes, preserve all original fields not directly related to the instruction (like personal info, config, etc.), and respond ONLY with the updated complete resume object in valid JSON format, enclosed within a single \`\`\`json ... \`\`\` block.`,
  },
  instructionPrompt: "", // For refinement instructions
  savedResumes: [], // Array to store { name: '...', jd: '...', resumeData: {...} }
};

export default DefaultResumeData;